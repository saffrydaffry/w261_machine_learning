{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science W261: Machine Learning at Scale\n",
    "Safyre Anderson\n",
    "\n",
    "safyre@berkeley.edu\n",
    "\n",
    "January 27, 2016 8am\n",
    "\n",
    "W261-3\n",
    "\n",
    "Week 2 HW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.0.  \n",
    "*What is a race condition in the context of parallel computation? Give an example.*\n",
    "\n",
    "*What is MapReduce?*\n",
    "\n",
    "*How does it differ from Hadoop?*\n",
    "\n",
    "*Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Race Condition**:\n",
    "\n",
    "A race condition refers to the issue of timing the completion and sequence of parallel jobs. For example, let's say there is one round of parrallel jobs is running and there is a subsequent round of parallel jobs. If not all the jobs from the first round of parallel jobs complete and the second round begins prematurely, the job will not complete correctly. This is known as a race condition. Hadoop HDFS was designed to abstract the prevention of race conditions and communication between parallel processes from developers.\n",
    "\n",
    "**MapReduce**\n",
    "\n",
    "MapReduce in the broadest sense is a functional programming-inpired parallel processing framework. It consists of at least two major steps: map and reduce (though subsequent MapReduce jobs can follow). \n",
    "\n",
    "**Hadoop**\n",
    "\n",
    "Hadoop is a Java-based framework for distributed data storage and processing that implements its own MapReduce framework. However, Hadoop is also a distributed file system that consists of a master 'namenode' as well as at least one 'datanode'. The namenode keeps track of where data are stored as well as the implementation on MapReduce Jobs. The datanode(s) store typically 3 copies of the data in chunks which are scattered between the datanodes for redundancy and disaster recovery.\n",
    "\n",
    "**Programming Paradigm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: source: not found\n",
      "/usr/local/hadoop\n"
     ]
    }
   ],
   "source": [
    "!source .bashrc\n",
    "!echo $HADOOP_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1. Sort in Hadoop MapReduce\n",
    "*Given as input: Records of the form `<integer, “NA”\\>`, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form <integer, “NA”> in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.*\n",
    "\n",
    "*Write code to generate N  random records of the form `<integer, “NA”>`. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used 1 node Hadoop cluster AWS instance:\n",
    "\n",
    "`alias aws_hadoop_master=\"ssh -i \"~/.ssh/***.pem\" ec2-user@ec2-54-213-63-253.us-west-2.compute.amazonaws.com\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rand_num.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rand_num.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "count = 0\n",
    "N = 10000\n",
    "while count < N:\n",
    "    print str(random.randint(0,N)) +\"\\t\" + \"NA\"\n",
    "    count +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm random_numbers.txt\n",
    "!python rand_num.py > random_numbers.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, I wanted to print out the top 10 rows and make sure there were only 10000 numbers (one on each line) generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8445\tNA\n",
      "7580\tNA\n",
      "4206\tNA\n",
      "2589\tNA\n",
      "5113\tNA\n",
      "4049\tNA\n",
      "7838\tNA\n",
      "3033\tNA\n",
      "4766\tNA\n",
      "5834\tNA\n",
      "10000 random_numbers.txt\n"
     ]
    }
   ],
   "source": [
    "!head random_numbers.txt\n",
    "!wc -l random_numbers.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml\thdfs-site.xml\t kms-site.xml\t     yarn-site.xml\r\n",
      "core-site.xml\t\thttpfs-site.xml  mapred-site.xml\r\n",
      "hadoop-policy.xml\tkms-acls.xml\t random_numbers.txt\r\n"
     ]
    }
   ],
   "source": [
    "!cp random_numbers.txt $HADOOP_HOME/input\n",
    "!ls $HADOOP_HOME/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop streaming should automatically sort the keys. Since our integers are the keys, we can just reprint them using the mapper to sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/26 09:20:01 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = ip-172-31-25-233.us-west-2.compute.internal/172.31.25.233\n",
      "STARTUP_MSG:   args = []\n",
      "STARTUP_MSG:   version = 2.7.1\n",
      "STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar\n",
      "STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z\n",
      "STARTUP_MSG:   java = 1.7.0_91\n",
      "************************************************************/\n",
      "16/01/26 09:20:01 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "16/01/26 09:20:01 INFO namenode.NameNode: createNameNode []\n",
      "16/01/26 09:20:12 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "16/01/26 09:20:15 INFO impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).\n",
      "16/01/26 09:20:15 INFO impl.MetricsSystemImpl: NameNode metrics system started\n",
      "16/01/26 09:20:15 INFO namenode.NameNode: fs.defaultFS is hdfs://localhost:9000\n",
      "16/01/26 09:20:15 INFO namenode.NameNode: Clients are to use localhost:9000 to access this namenode/service.\n",
      "16/01/26 09:20:28 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070\n",
      "16/01/26 09:20:31 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "16/01/26 09:20:33 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n",
      "16/01/26 09:20:33 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\n",
      "16/01/26 09:20:33 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "16/01/26 09:20:34 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\n",
      "16/01/26 09:20:34 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n",
      "16/01/26 09:20:34 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n",
      "16/01/26 09:20:36 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\n",
      "16/01/26 09:20:36 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\n",
      "16/01/26 09:20:37 INFO http.HttpServer2: Jetty bound to port 50070\n",
      "16/01/26 09:20:37 INFO mortbay.log: jetty-6.1.26\n",
      "16/01/26 09:20:45 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070\n",
      "16/01/26 09:20:47 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\n",
      "16/01/26 09:20:47 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\n",
      "16/01/26 09:20:49 INFO namenode.FSNamesystem: No KeyProvider found.\n",
      "16/01/26 09:20:49 INFO namenode.FSNamesystem: fsLock is fair:true\n",
      "16/01/26 09:20:50 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000\n",
      "16/01/26 09:20:50 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "16/01/26 09:20:50 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "16/01/26 09:20:50 INFO blockmanagement.BlockManager: The block deletion will start around 2016 Jan 26 09:20:50\n",
      "16/01/26 09:20:50 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "16/01/26 09:20:50 INFO util.GSet: VM type       = 64-bit\n",
      "16/01/26 09:20:50 INFO util.GSet: 2.0% max memory 966.7 MB = 19.3 MB\n",
      "16/01/26 09:20:50 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "16/01/26 09:20:51 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n",
      "16/01/26 09:20:51 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
      "16/01/26 09:20:51 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "16/01/26 09:20:51 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "16/01/26 09:20:51 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "16/01/26 09:20:51 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false\n",
      "16/01/26 09:20:51 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n",
      "16/01/26 09:20:51 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "16/01/26 09:20:51 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "16/01/26 09:20:51 INFO namenode.FSNamesystem: fsOwner             = ubuntu (auth:SIMPLE)\n",
      "16/01/26 09:20:51 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "16/01/26 09:20:51 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "16/01/26 09:20:51 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "16/01/26 09:20:51 INFO namenode.FSNamesystem: Append Enabled: true\n",
      "16/01/26 09:20:52 INFO util.GSet: Computing capacity for map INodeMap\n",
      "16/01/26 09:20:52 INFO util.GSet: VM type       = 64-bit\n",
      "16/01/26 09:20:52 INFO util.GSet: 1.0% max memory 966.7 MB = 9.7 MB\n",
      "16/01/26 09:20:52 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "16/01/26 09:20:52 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "16/01/26 09:20:52 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "16/01/26 09:20:52 INFO namenode.FSDirectory: Maximum size of an xattr: 16384\n",
      "16/01/26 09:20:52 INFO namenode.NameNode: Caching file names occuring more than 10 times\n",
      "16/01/26 09:20:53 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "16/01/26 09:20:53 INFO util.GSet: VM type       = 64-bit\n",
      "16/01/26 09:20:53 INFO util.GSet: 0.25% max memory 966.7 MB = 2.4 MB\n",
      "16/01/26 09:20:53 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "16/01/26 09:20:53 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "16/01/26 09:20:53 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0\n",
      "16/01/26 09:20:53 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000\n",
      "16/01/26 09:20:53 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "16/01/26 09:20:53 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "16/01/26 09:20:53 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "16/01/26 09:20:53 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "16/01/26 09:20:53 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "16/01/26 09:20:53 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "16/01/26 09:20:53 INFO util.GSet: VM type       = 64-bit\n",
      "16/01/26 09:20:53 INFO util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB\n",
      "16/01/26 09:20:53 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "16/01/26 09:20:53 INFO common.Storage: Lock on /usr/local/hadoop/hadoop_data/hdfs/namenode/current/in_use.lock acquired by nodename 9106@ip-172-31-25-233.us-west-2.compute.internal\n",
      "16/01/26 09:20:58 INFO namenode.FileJournalManager: Recovering unfinalized segments in /usr/local/hadoop/hadoop_data/hdfs/namenode/current/current\n",
      "16/01/26 09:21:01 INFO namenode.FileJournalManager: Finalizing edits file /usr/local/hadoop/hadoop_data/hdfs/namenode/current/current/edits_inprogress_0000000000000000069 -> /usr/local/hadoop/hadoop_data/hdfs/namenode/current/current/edits_0000000000000000069-0000000000000000069\n",
      "16/01/26 09:21:04 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\n",
      "16/01/26 09:21:05 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 3 seconds.\n",
      "16/01/26 09:21:05 INFO namenode.FSImage: Loaded image for txid 0 from /usr/local/hadoop/hadoop_data/hdfs/namenode/current/current/fsimage_0000000000000000000\n",
      "16/01/26 09:21:05 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3749dde5 expecting start txid #1\n",
      "16/01/26 09:21:05 INFO namenode.FSImage: Start loading edits file /usr/local/hadoop/hadoop_data/hdfs/namenode/current/current/edits_0000000000000000001-0000000000000000068\n",
      "16/01/26 09:21:05 INFO namenode.EditLogInputStream: Fast-forwarding stream '/usr/local/hadoop/hadoop_data/hdfs/namenode/current/current/edits_0000000000000000001-0000000000000000068' to transaction ID 1\n",
      "16/01/26 09:21:07 INFO namenode.FSEditLogLoader: replaying edit log: 2/68 transactions completed. (3%)\n",
      "16/01/26 09:21:09 INFO namenode.FSEditLogLoader: replaying edit log: 26/68 transactions completed. (38%)\n",
      "16/01/26 09:21:09 INFO namenode.FSImage: Edits file /usr/local/hadoop/hadoop_data/hdfs/namenode/current/current/edits_0000000000000000001-0000000000000000068 of size 1048576 edits # 68 loaded in 3 seconds\n",
      "16/01/26 09:21:09 INFO namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@528701be expecting start txid #69\n",
      "16/01/26 09:21:09 INFO namenode.FSImage: Start loading edits file /usr/local/hadoop/hadoop_data/hdfs/namenode/current/current/edits_0000000000000000069-0000000000000000069\n",
      "16/01/26 09:21:09 INFO namenode.EditLogInputStream: Fast-forwarding stream '/usr/local/hadoop/hadoop_data/hdfs/namenode/current/current/edits_0000000000000000069-0000000000000000069' to transaction ID 1\n",
      "16/01/26 09:21:09 INFO namenode.FSImage: Edits file /usr/local/hadoop/hadoop_data/hdfs/namenode/current/current/edits_0000000000000000069-0000000000000000069 of size 1048576 edits # 1 loaded in 0 seconds\n",
      "16/01/26 09:21:09 INFO namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)\n",
      "16/01/26 09:21:09 INFO namenode.FSImage: Save namespace ...\n",
      "16/01/26 09:21:10 INFO namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0\n",
      "16/01/26 09:21:10 INFO namenode.FSEditLog: Starting log segment at 70\n",
      "16/01/26 09:21:11 INFO namenode.NameCache: initialized with 0 entries 0 lookups\n",
      "16/01/26 09:21:11 INFO namenode.FSNamesystem: Finished loading FSImage in 18383 msecs\n",
      "16/01/26 09:21:16 INFO namenode.NameNode: RPC server is binding to localhost:9000\n",
      "16/01/26 09:21:16 INFO ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue\n",
      "16/01/26 09:21:16 INFO ipc.Server: Starting Socket Reader #1 for port 9000\n",
      "16/01/26 09:21:18 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean\n",
      "16/01/26 09:21:19 INFO namenode.LeaseManager: Number of blocks under construction: 0\n",
      "16/01/26 09:21:19 INFO namenode.LeaseManager: Number of blocks under construction: 0\n",
      "16/01/26 09:21:19 INFO hdfs.StateChange: STATE* Safe mode ON. \n",
      "The reported blocks 0 needs additional 6 blocks to reach the threshold 0.9990 of total blocks 6.\n",
      "The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.\n",
      "16/01/26 09:21:19 INFO blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0\n",
      "16/01/26 09:21:20 INFO ipc.Server: IPC Server Responder: starting\n",
      "16/01/26 09:21:20 INFO namenode.NameNode: NameNode RPC up at: localhost/127.0.0.1:9000\n",
      "16/01/26 09:21:20 INFO namenode.FSNamesystem: Starting services required for active state\n",
      "16/01/26 09:21:20 INFO ipc.Server: IPC Server listener on 9000: starting\n",
      "16/01/26 09:21:20 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\n",
      "^C16/01/26 09:51:01 ERROR namenode.NameNode: RECEIVED SIGNAL 2: SIGINT\n",
      "16/01/26 09:51:01 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at ip-172-31-25-233.us-west-2.compute.internal/172.31.25.233\n",
      "************************************************************/\n",
      "\n",
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-ubuntu-resourcemanager-ip-172-31-25-233.out\n",
      "localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-ubuntu-nodemanager-ip-172-31-25-233.out\n"
     ]
    }
   ],
   "source": [
    "!hdfs namenode && hdfs datanode\n",
    "!$HADOOP_HOME/sbin/start-yarn.sh\n",
    "\n",
    "# CD to hadoop root dir\n",
    "!cd $HADOOP_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 0 (0 B)\n",
      "Present Capacity: 0 (0 B)\n",
      "DFS Remaining: 0 (0 B)\n",
      "DFS Used: 0 (0 B)\n",
      "DFS Used%: NaN%\n",
      "Under replicated blocks: 0\n",
      "Blocks with corrupt replicas: 0\n",
      "Missing blocks: 0\n",
      "Missing blocks (with replication factor 1): 0\n",
      "\n",
      "-------------------------------------------------\n",
      "16/01/25 15:47:32 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "Total Nodes:1\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "ec2-54-213-63-253.us-west-2.compute.amazonaws.com:35267\t        RUNNING\tec2-54-213-63-253.us-west-2.compute.amazonaws.com:8042\t                           0\n"
     ]
    }
   ],
   "source": [
    "# check hdfs status and yarn nodes\n",
    "!hdfs dfsadmin -report\n",
    "!yarn node -list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HDFS health:\n",
    "\n",
    "<http:/ec2-54-213-63-253.us-west-2.compute.amazonaws.com:50070/dfshealth.html#tab-overview>\n",
    "\n",
    "For YARN cluster/job manager:\n",
    "http://ec2-54-213-63-253.us-west-2.compute.amazonaws.com:8088/cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "# Simply read and print out key value pairs from input\n",
    "# note, writing to standard out seems more secure than print\n",
    "for line in sys.stdin:\n",
    "    sys.stdout.write(line)\n",
    "    #key, empty_string = line.split('\\t')\n",
    "    #print str(key) +\"\\t\" + empty_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "# Simply read and print out key value pairs from input\n",
    "for line in sys.stdin:\n",
    "    #key, empty_string = line.split('\\t')\n",
    "    #print str(key) +\"\\t\" + empty_string\n",
    "    sys.stdout.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tNA\n",
      "1\tNA\n",
      "1\tNA\n",
      "2\tNA\n",
      "5\tNA\n",
      "6\tNA\n",
      "9\tNA\n",
      "10\tNA\n",
      "10\tNA\n",
      "11\tNA\n",
      "9989\tNA\n",
      "9989\tNA\n",
      "9992\tNA\n",
      "9995\tNA\n",
      "9995\tNA\n",
      "9997\tNA\n",
      "9999\tNA\n",
      "9999\tNA\n",
      "9999\tNA\n",
      "10000\tNA\n"
     ]
    }
   ],
   "source": [
    "!./mapper.py <random_numbers.txt| sort -n | ./reducer.py >rand_num.out\n",
    "!head rand_num.out\n",
    "!tail rand_num.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [/tmp/hadoop-unjar1215359286937913206/] [] /tmp/streamjob4040146808467195045.jar tmpDir=null\n",
      "16/01/26 04:23:46 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 04:23:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/01/26 04:24:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/26 04:24:04 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/01/26 04:24:13 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453781403055_0002\n",
      "16/01/26 04:24:23 INFO impl.YarnClientImpl: Submitted application application_1453781403055_0002\n",
      "16/01/26 04:24:27 INFO mapreduce.Job: The url to track the job: http://ip-172-31-25-233:8088/proxy/application_1453781403055_0002/\n",
      "16/01/26 04:24:27 INFO mapreduce.Job: Running job: job_1453781403055_0002\n",
      "16/01/26 04:27:48 INFO mapreduce.Job: Job job_1453781403055_0002 running in uber mode : false\n",
      "16/01/26 04:27:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/26 04:30:09 INFO ipc.Client: Retrying connect to server: ip-172-31-25-233.us-west-2.compute.internal/172.31.25.233:50272. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)\n",
      "16/01/26 04:30:10 INFO ipc.Client: Retrying connect to server: ip-172-31-25-233.us-west-2.compute.internal/172.31.25.233:50272. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)\n",
      "16/01/26 04:30:11 INFO ipc.Client: Retrying connect to server: ip-172-31-25-233.us-west-2.compute.internal/172.31.25.233:50272. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)\n",
      "16/01/26 04:30:12 INFO ipc.Client: Retrying connect to server: ip-172-31-25-233.us-west-2.compute.internal/172.31.25.233:50272. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)\n",
      "16/01/26 04:30:13 INFO ipc.Client: Retrying connect to server: ip-172-31-25-233.us-west-2.compute.internal/172.31.25.233:50272. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)\n",
      "16/01/26 04:30:14 INFO ipc.Client: Retrying connect to server: ip-172-31-25-233.us-west-2.compute.internal/172.31.25.233:50272. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=3, sleepTime=1000 MILLISECONDS)\n",
      "16/01/26 04:31:54 INFO mapreduce.Job: Job job_1453781403055_0002 failed with state FAILED due to: Application application_1453781403055_0002 failed 2 times due to AM Container for appattempt_1453781403055_0002_000002 exited with  exitCode: 1\n",
      "For more detailed output, check application tracking page:http://ip-172-31-25-233:8088/cluster/app/application_1453781403055_0002Then, click on links to logs of each attempt.\n",
      "Diagnostics: Exception from container-launch.\n",
      "Container id: container_1453781403055_0002_02_000001\n",
      "Exit code: 1\n",
      "Stack trace: ExitCodeException exitCode=1: \n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:545)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:456)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)\n",
      "\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)\n",
      "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)\n",
      "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "\n",
      "Container exited with a non-zero exit code 1\n",
      "Failing this attempt. Failing the application.\n",
      "16/01/26 04:31:56 INFO mapreduce.Job: Counters: 0\n",
      "16/01/26 04:31:56 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/hadoop-*streaming*.jar -mapper $HADOOP_HOME/mapper.py -reducer reducer.py -input /user/safyre/input/random_numbers.txt -output /user/safyre/output/SortOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.  WORDCOUNT\n",
    "*Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that\n",
    "will determine the word count (number of occurrences) of each white-space delimitted token \n",
    "(assume spaces, fullstops, comma as delimiters). Examine the word “assistance” \n",
    "and report its word count results.*\n",
    "\n",
    " \n",
    "CROSSCHECK: `>grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l`    \n",
    "\n",
    "    `8`    \n",
    "\n",
    "*#NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? \n",
    "10 times! This is the number we are looking for!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import re, string\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# store a regex expression into a pattern object\n",
    "# that seeks words including underscores and single quotes\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "translate_table = string.maketrans(\"\",\"\") #empty translation\n",
    "\n",
    "# file input\n",
    "filename = sys.argv[1]\n",
    "\n",
    "# list of words argument, '*' means all words\n",
    "word_list = sys.argv[2]\n",
    "count = 0\n",
    "\n",
    "with open(filename, 'rU') as f:\n",
    "    for line in f.readlines():\n",
    "        #strip punctuation from line\n",
    "        line = line.translate(translate_table, string.punctuation)\n",
    "        \n",
    "        # if not all words selected,\n",
    "        # go through each word in word list and count occurances\n",
    "        if word_list != '*':\n",
    "            for word in word_list.split():\n",
    "                counts = [int(1) if (x == word) and (word.isalpha()) else int(0) for x in WORD_RE.findall(line)]\n",
    "                counts = np.array(counts)\n",
    "            \n",
    "                if counts.sum() > 0:\n",
    "                    print word + \"\\t\" + str(counts.sum())\n",
    "        else: \n",
    "            for word in line.split():\n",
    "                if word.isalpha():\n",
    "                    print word + \"\\t\"+ str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "# recycled code from W205, sorry\n",
    "import re, string\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#if a new word enters the fray, print the current word and its counts\n",
    "def wcount(prev_word ,counts):\n",
    "    if prev_word is not None:\n",
    "            print(prev_word + \"\\t\" + str(counts))\n",
    "\n",
    "prev_word = None\n",
    "counts = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    word, value =line.split(\"\\t\",1)\n",
    "    if word != prev_word:\n",
    "        wcount(prev_word, counts)\n",
    "        prev_word = word \n",
    "        counts = 0\n",
    "    counts += eval(value)\n",
    "\n",
    "# A print just for the final word\n",
    "wcount(prev_word, counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py\n",
    "!./mapper.py enronemail_1h.txt assistance |sort| ./reducer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW2.2.1  \n",
    "\n",
    "*Using Hadoop MapReduce and your wordcount job (from HW2.2) \n",
    "determine the top-10 occurring tokens (most frequent tokens)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./mapper.py enronemail_1h.txt \\* |sort | ./reducer.py |sort -k 2n >hw221.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\t260\r\n",
      "for\t373\r\n",
      "your\t391\r\n",
      "in\t415\r\n",
      "you\t427\r\n",
      "a\t529\r\n",
      "of\t560\r\n",
      "and\t662\r\n",
      "to\t961\r\n",
      "the\t1246\r\n"
     ]
    }
   ],
   "source": [
    "# sorted in ascending order\n",
    "!tail hw221.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "*Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:*\n",
    "\n",
    "   `the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM` \n",
    "\n",
    "   *E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then $Pr(X=“assistance”|Y=SPAM) = 5/1000$. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since $log(xy) = log(x) + log(y)$, it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report.* \n",
    "\n",
    "   *Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the  posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see.* \n",
    "\n",
    "   *Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:*\n",
    "\n",
    "*Let DF represent the evalution set in the following:*\n",
    "$Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|$\n",
    "\n",
    "*Where $||$ denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import re, string\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# store a regex expression into a pattern object\n",
    "# that seeks words including underscores and single quotes\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "TRUTH_RE = re.compile(r\"\\t(\\d)\\t\")\n",
    "translate_table = string.maketrans(\"\",\"\") #empty translation\n",
    "\n",
    "\n",
    "# file input\n",
    "filename = sys.argv[1]\n",
    "\n",
    "# for this part, just assume word_list is length 1\n",
    "word_list = sys.argv[2]\n",
    "\n",
    "# Avoid KeyError if no data in chunk\n",
    "#counts_dict = dict.fromkeys(['0', '1'], 0)\n",
    "counts_dict = {}\n",
    "doc_len    = 0\n",
    "spam_count = 0\n",
    "ham_count  = 0\n",
    "\n",
    "line_count = 0\n",
    "with open(filename, 'rU') as f:\n",
    "    for line in f.readlines():\n",
    "        \n",
    "        # Parse out TRUTH\n",
    "        # truth is the actual label provided in the data\n",
    "        # 1 = spam, 0 = ham\n",
    "        #key is the id of the emai\n",
    "        key = line.split()[0]\n",
    "        try:\n",
    "            truth = TRUTH_RE.findall(line)[0]\n",
    "\n",
    "        except:\n",
    "            #for some reason line 59 gives problems\n",
    "            # truth = '1'\n",
    "            continue\n",
    "        \n",
    "        # Remove punctuation\n",
    "        line = line.translate(translate_table, string.punctuation)\n",
    "        \n",
    "        '''\n",
    "         # define empty dictionaries\n",
    "        for category in ['0','1']:\n",
    "            counts_dict[category] = {}\n",
    "        '''\n",
    "       \n",
    "        if word_list != \"*\":\n",
    "            for word in word_list.split():\n",
    "                #doc_len = len(line.split())\n",
    "                counts = [1 if (x == word) and (x.isalpha()) else 0 for x in WORD_RE.findall(line)]\n",
    "                counts = np.array(counts)  \n",
    "                \n",
    "                # Only pass to reducer if the word is present\n",
    "                if counts.sum() > 0:\n",
    "                    count = counts.sum()\n",
    "                    #counts_dict[truth][word] = counts_dict[truth].get(word, 0) + int(count)\n",
    "                    print key + \"\\t\" + truth + \"\\t\" + word + \"\\t\" + str(count) #+ \"\\t\" + str(doc_len)\n",
    "\n",
    "        else:\n",
    "            for word in list(set(line.split())):\n",
    "                counts = [1 if (x == word) and (x.isalpha()) else 0 for x in WORD_RE.findall(line)]\n",
    "                counts = np.array(counts)\n",
    "                \n",
    "                count = counts.sum()\n",
    "                #counts_dict[truth][word] = counts_dict[truth].get(word, 0) + int(count)\n",
    "                print key +\"\\t\" + truth + \"\\t\" + word + \"\\t\" + str(count) #+ \"\\t\" + str(doc_len)\n",
    "\n",
    "'''\n",
    "for category, word_dictionary in counts_dict.iteritems():\n",
    "    for words, count in counts_dict[category].iteritems():\n",
    "        print key + category + \"\\t\" + words + \"\\t\" + str(count) + \"\\t\" + str(doc_len)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import re, sys\n",
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "## training, gather all the counts and calculate corpus-wide priors, etc\n",
    "## data come in as strings, \n",
    "## ID TRUTH WORD COUNT \n",
    "# define empty dictionaries\n",
    "\n",
    "\n",
    "# number of documents in each class\n",
    "N_spam_docs = 0\n",
    "N_ham_docs  = 0\n",
    "\n",
    "# number of terms in each class\n",
    "N_spam_terms = 0\n",
    "N_ham_terms = 0\n",
    "counts_dict ={}\n",
    "keys_dict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    key, truth, word, count = line.split()\n",
    "    keys_dict[key+\"_\"+truth] = {}\n",
    "    keys_dict[key+\"_\"+truth][word] = {}\n",
    "    \n",
    "    for i in ['0', '1']:\n",
    "        counts_dict[i] = {}\n",
    "    # tabulate word counts for each class\n",
    "    counts_dict[truth][word] = counts_dict[truth].get(word, 0) + int(count)\n",
    "    \n",
    "    if truth == '1':\n",
    "        N_spam_docs  += 1\n",
    "        N_spam_terms += int(count)\n",
    "    else:\n",
    "        N_ham_docs   += 1\n",
    "        N_ham_terms += int(count)\n",
    "        \n",
    "priors = {'0': float(N_ham_docs)/(N_spam_docs+N_ham_docs),\n",
    "          '1': float(N_spam_docs)/(N_spam_docs+N_ham_docs)}\n",
    "\n",
    "prior_counts = {'0': float(N_ham_terms),\n",
    "          '1': float(N_spam_terms)}\n",
    "\n",
    "## Calculate conditional probabilities\n",
    "## P(word | class) \n",
    "posteriors = {}\n",
    "for category in ['0', '1']:\n",
    "    posteriors[category] = {}\n",
    "    for word in counts_dict[category].keys():\n",
    "        posteriors[category][word] = float(counts_dict[category][word])/float(prior_counts[category])\n",
    "\n",
    "for category in ['0', '1']:\n",
    "    for key, word in keys_dict.iteritems():\n",
    "        print key.split('_')[0] + \"\\t\" +key.split('_')[1] + \"\\t\" + str(word) + '\\t' +str(priors['0']) + \\\n",
    "        '\\t' + str(priors['1']) + '\\t' + str(posteriors['0'][word]) + '\\t' + str(posteriors['1'][word])\n",
    "        #keys_dict[key][word]['prior_ham']    = priors['0']\n",
    "        #keys_dict[key][word]['prior_spam']   = priors['1']\n",
    "        #keys_dict[key][word]['posterior_ham'] = posteriors['0'][word]\n",
    "        #keys_dict[key][word]['posterior_spam'] = posteriors['1'][word]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "#print \"Priors are: \"\n",
    "#for category in priors:\n",
    "#    print category + \" \" + str(priors[category]) + \"n = \" +str(prior_counts[category]) \n",
    "\n",
    "spam_vocab = counts_dict['1'].keys()\n",
    "ham_vocab  = counts_dict['0'].keys()\n",
    "\n",
    "spam_vocab_n = len(counts_dict['1'].keys())\n",
    "ham_vocab_n  = len(counts_dict['0'].keys())\n",
    "\n",
    "# all unique words from both classes\n",
    "vocab = list(set(counts_dict['0'].keys()).union(counts_dict['1'].keys()))\n",
    "len_vocab = len(vocab)\n",
    "\n",
    "#print \"\\nPosteriors are: \"\n",
    "#for category in posteriors:\n",
    "#    for word in posteriors[category]:\n",
    "#        print word + \" in class \" + category + \" \" + str(posteriors[category][word]) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "\n",
    "## Testing the classifer\n",
    "## Without laplacian transform \n",
    "#print \"DOC_ID | TRUTH | CLASS \"\n",
    "#print \"=======================\\n\"\n",
    "\n",
    "\n",
    "def cum_log_probs(prev_key, label, prediction):# ham_score, spam_score):\n",
    "    if prev_key is not None:\n",
    "        print prev_key + \"\\t\" + label + \"\\t\" +str(prediction)# +  \" \" +ham_score + \" \" + spam_score\n",
    "\n",
    "prev_key = None\n",
    "counts = 0\n",
    "prediction =0\n",
    "correct = 0\n",
    "\n",
    "'''\n",
    "for line in sys.stdin:\n",
    "    word, value =line.split(\"\\t\",1)\n",
    "    if word!=prev_word:\n",
    "        wcount(prev_word, counts)\n",
    "        prev_word = word \n",
    "        counts = 0\n",
    "    counts += eval(value)\n",
    "\n",
    "wcount(prev_word, counts)\n",
    "'''\n",
    "\n",
    "# This could probably be another MapReduce job...\n",
    "for line in sys.stdin:\n",
    "    for key, truth, word, prior_ham, prior_spam, posterior_ham, posterior_spam in line.split():\n",
    "        print key, truth, word, count\n",
    "        if key != prev_key:\n",
    "            # Dump the previous key's statistics\n",
    "            if prev_key is not None:\n",
    "                if int(prediction) == int(truth):\n",
    "                    correct +=1\n",
    "                #print prev_key + \"\\t\" + truth + \"\\t\" +str(prediction) +  \" \" +str(score[0]) + \" \" + str(score[1])\n",
    "                cum_logs_prob(prev_key, truth, prediction)# str(score[0]), str(score[1]))\n",
    "            #initialize scores for new sample\n",
    "            score = [0,0]\n",
    "            score[0]= log(prior_ham) + log(float(posterior_ham))\n",
    "            score[1] = log(prior_spam) + log(float(posterior_spam))\n",
    "            #for category in ['0', '1']:\n",
    "            #    idx = int(category)\n",
    "            #    score[idx] =  log(priors[category])\n",
    "                    #score[idx] += log(float(posteriors[category][word]))\n",
    "                    \n",
    "            prev_key = key\n",
    "            \n",
    "        else:\n",
    "            score[0] += log(float(posterior_ham))\n",
    "            score[1] += log(float(posterior_spam))\n",
    "            #for category in ['0', '1']:\n",
    "            #    for word in posteriors[category]:\n",
    "            #        score[idx] += log(float(posteriors[category][word]))\n",
    "            \n",
    "            score = np.array(score)\n",
    "            prediction = score.argmax()\n",
    "            \n",
    "#print prev_key + \"\\t\" + truth + \"\\t\" +str(prediction) +  \" \" +str(score[0]) + \" \" + str(score[1])\n",
    "cum_log_probs(prev_key, truth, prediction)# str(score[0]), str(score[1]))\n",
    "\n",
    "accuracy = float(correct)/(N_spam_docs+N_ham_docs)*100.0\n",
    "print \"Accuracy: \", accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./reducer2.py: 8: ./reducer2.py: Syntax error: \"(\" unexpected\n",
      "Traceback (most recent call last):\n",
      "  File \"./reducer.py\", line 56, in <module>\n",
      "    '\\t' + str(priors['1']) + '\\t' + str(posteriors['0'][word]) + '\\t' + str(posteriors['1'][word])\n",
      "TypeError: unhashable type: 'dict'\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x mapper.py\n",
    "!chmod a+x reducer.py\n",
    "!chmod a+x reducer2.py\n",
    "!./mapper.py enronemail_1h.txt assistance | sort | ./reducer.py |./reducer2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.4 \n",
    "*Repeat HW2.3 with the following modification: use Laplace plus-one smoothing.* \n",
    "*Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.*\n",
    "\n",
    "*For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:*\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "*OR the original paper by the curators of the Enron email data:*\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.5. \n",
    "*Repeat HW2.4. This time when modeling and classification ignore \n",
    "tokens with a frequency of less than three (3) in the training set. \n",
    "How does it affect the misclassifcation error of learnt naive multinomial \n",
    "Bayesian Classifier on the training dataset:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.6\n",
    "\n",
    "*Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm*\n",
    "\n",
    "*It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn.*\n",
    "*The Machine Learning toolkit available in Python. In this exercise, \n",
    "we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  \n",
    "For more information on this implementation see:* http://scikit-learn.org/stable/modules/naive_bayes.html   \n",
    "\n",
    "*In this exercise, please complete the following:*\n",
    "\n",
    "- *Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)*\n",
    "- *Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error*\n",
    "- *Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sklearn_run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sklearn_run.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"enronemail_1h.txt\",sep='\\t',header=None)\n",
    "data.columns = ['ID', 'TRUTH', 'SUBJECT', 'TEXT']\n",
    "data = data.replace(np.nan,' ', regex=True)\n",
    "train_data , train_labels = data['SUBJECT']+data['TEXT'] , data['TRUTH']\n",
    "vec = CountVectorizer()\n",
    "vec_t = vec.fit_transform(train_data)\n",
    "\n",
    "#Fit and predict Naivebayes\n",
    "clf = MultinomialNB(alpha = 1.0)       \n",
    "clf.fit(vec_t, train_labels)\n",
    "y_pred = clf.predict(vec_t)\n",
    "err = 1- metrics.accuracy_score(train_labels, y_pred)\n",
    "print \"Training Error: \" + str(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"./sklearn_run.py\", line 24, in <module>\n",
      "    clf.fit(vec_t, train_labels)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/naive_bayes.py\", line 531, in fit\n",
      "    Y = labelbin.fit_transform(y)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/base.py\", line 455, in fit_transform\n",
      "    return self.fit(X, **fit_params).transform(X)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/label.py\", line 308, in fit\n",
      "    self.classes_ = unique_labels(y)\n",
      "  File \"/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/utils/multiclass.py\", line 99, in unique_labels\n",
      "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
      "ValueError: Unknown label type: (array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0,\n",
      "       0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0,\n",
      "       1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n",
      "       1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,\n",
      "       0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, ' ', 1.0, 1.0, 1.0, 0.0, 0.0,\n",
      "       0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0,\n",
      "       1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0,\n",
      "       1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0], dtype=object),)\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x sklearn_run.py\n",
    "!./sklearn_run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 2.6.1 OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "-  *Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW2.6 and report the misclassification error* \n",
    "-  *Discuss the performance differences in terms of misclassification error rates over the dataset in HW2.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn. Why such big differences. Explain.* \n",
    "\n",
    "*Which approach to Naive Bayes would you recommend for SPAM detection? Justify your selection.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.7 OPTIONAL (note this exercise is a stretch HW and optional)\n",
    "\n",
    "*The Enron SPAM data in the following folder enron1-Training-Data-RAW is in raw text form (with subfolders for SPAM and HAM that contain raw email messages in the following form:*\n",
    "\n",
    "- Line 1 contains the subject\n",
    "- The remaining lines contain the body of the email message.\n",
    "\n",
    "*In Python write a script to produce a TSV file called train-Enron-1.txt that has a similar format as the enronemail_1h.txt that you have been using so far. Please pay attend to funky characters and tabs. Check your resulting formated email data in Excel and in Python (e.g., count up the number of fields in each row; the number of SPAM mails and the number of HAM emails). Does each row correspond to an email record with four values? Note: use \"NA\" to denote empty field values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.8 OPTIONAL\n",
    "*Using Hadoop Map-Reduce write job(s) to perform the following:*\n",
    " - *Train a multinomial Naive Bayes Classifier with Laplace plus one smoothing using the data extracted in HW2.7 (i.e., train-Enron-1.txt). Use all white-space delimitted tokens as independent input variables \n",
    "(assume spaces, fullstops, commas as delimiters). Drop tokens with a frequency of less than three (3).*\n",
    " - *Test the learnt classifier using enronemail_1h.txt and report the misclassification error rate. \n",
    "    Remember to use all white-space delimitted tokens as independent input variables \n",
    "    (assume spaces, fullstops, commas as delimiters). \n",
    "    How do we treat tokens in the test set that do not appear in the training set?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.8.1 OPTIONAL\n",
    "-  *Run  both the Multinomial Naive Bayes and the Bernoulli Naive Bayes algorithms \n",
    "from SciKit-Learn (using default settings) over the same training data used in \n",
    "HW2.8 and report the misclassification error on both the training set and the testing set*\n",
    "- *Prepare a table to present your results, where rows correspond to approach used (SciKit-Learn Multinomial NB; SciKit-Learn Bernouili NB; Your Hadoop implementation)  and the columns presents the training misclassification error, and the misclassification error on the test data set*\n",
    "- *Discuss the performance differences in terms of misclassification error rates over the test and training datasets by the different implementations. Which approch (Bernouili versus Multinomial) would you recommend for SPAM detection? Justify your selection.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
