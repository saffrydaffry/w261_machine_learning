{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## W261 Week5 HW\n",
    "\n",
    "#### Names Safyre Anderson, Howard Wen , Vamsi Sakhamuri\n",
    "\n",
    "#### Emails safyre@berkeley.edu, howard.wen1@gmail.com, vamsi@ischool.berkeley.edu\n",
    "\n",
    "#### Time of Initial Submission: February 18th, 2016 8am PST\n",
    "\n",
    "#### Section W261-3, Spring 2016\n",
    "\n",
    "#### Week 5 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.0\n",
    "*What is a data warehouse? What is a Star schema? When is it used?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Data Warehouse:*\n",
    "\n",
    "https://en.wikipedia.org/wiki/Data_warehouse. A data warehouse is an enterprise-level repository for historical and current datasets. Typically, a business has many sources of data that need to be processed for a variety of applications: e.g. reporting, business analyics, and/ or predictive analytics.  For example, in mobile advertising data pipelines, an ad request from an app on a customer's mobile device will require a decision that involves streaming usage data in addition to historical data. The historical data would have been used to determine which types of ads are relevant to the end user, while the streaming data used to generate a prediction. Data collected from these different sources and various purposes need to be passed through some form of transformation and storage in order to be processed in subsequent parts of the pipeline. The gathering of data, transforming them and storing them is known as ETL or extract, transform, and load. The loading piece of ETL refers to storing processed data into the data warehouse--typically, a relational database, but can also include semi-stuctured and unstructured data storage.\n",
    "\n",
    "\n",
    "Two traditional examples of data warehousing strategies are OLAP (online analytical processing) and OLTP (online transactional processing).\n",
    "\n",
    "2. *Star Schema:*\n",
    "\n",
    "The star schema is a framework for organizing data in a relational database where a central fact table acts as a means to connect data between different dimensional tables. The fact table will contain foreign keys for each of the branching tables. Additionally, each of the branching tables focusses on data related to one dimension that an analysis could splice on. For example an ecommerce schema might have a transactions-based fact table that contains foreign keys for a customers table, and a payment table. The transactions table will be able to join with both the customers and payment tables directly, but the payment table and customer table would need to join with each other indirectly through the transactions table since they would not (in this example) contain each others foreign keys. \n",
    "\n",
    "3. *When is it used?:*\n",
    "\n",
    "The star schema is useful for highly structured data that may have numerous dimensions that could be used to break down the data during analyses. The main reason this is useful in most reporting applications is because it makes querying data much more efficient. Instead of having one large table with all the data and many rows, joining a smaller table with a larger one can shrink the size of the data significantly before it is extracted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.1\n",
    "1. *In the database world What is 3NF? Does machine learning use data in 3NF? If so why? In what form does ML consume data?*\n",
    "\n",
    "3NF stands for \"Third normal form\". This refers to how data are organized with their respective tables and keys: Attributes in a table should provide a fact about the table's key and nothing else (\"Nothing but the key\" https://en.wikipedia.org/wiki/Third_normal_form). This structure allows a database to avoid duplicating data as much as possible and therefore save storage space. Machine learning would not be able to utilize data in this form very well--features would be scattered across different sources so the classifier would not be able to train on features in different tables. ML classifiers need to have all the data they depend on in one location (distributed or not) and therefore need the data to be denormalized.\n",
    "\n",
    "2. *Why would one use log files that are denormalized?*\n",
    "\n",
    "Log files are normally stored as semi-structured or unstructured data. They may have some regular format, but for the most part, log files contain plain English that would need to be parsed during analysis. If log files were placed into a structure and normalized, closely related pieces of information that could be useful would be separated from each other--for instance, an error message with a time stamp. Furthermore, a denormalized log file is more usable because log files are all text. Normalized data typically contains data that can be strongly typed, but if a log file is more intuitively parsed as a document then normalization may just overcomplicate things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW 5.2\n",
    "*Using MRJob, implement a hashside join (memory-backed map-side) for left, \n",
    "right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)*\n",
    ":\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "(1) Left joining Table Left with Table Right\n",
    "(2) Right joining Table Left with Table Right\n",
    "(3) Inner joining Table Left with Table Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-02-15 14:26:38--  http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/anonymous-msweb.data\n",
      "Resolving archive.ics.uci.edu... 128.195.10.249\n",
      "Connecting to archive.ics.uci.edu|128.195.10.249|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1423098 (1.4M) [text/plain]\n",
      "Saving to: 'anonymous-msweb.data'\n",
      "\n",
      "anonymous-msweb.dat 100%[=====================>]   1.36M  1.91MB/s   in 0.7s   \n",
      "\n",
      "2016-02-15 14:26:39 (1.91 MB/s) - 'anonymous-msweb.data' saved [1423098/1423098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## first get 4.2 data and preprocess it:\n",
    "!wget -O anonymous-msweb.data http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/anonymous-msweb.data\n",
    "\n",
    "## python code to reformat the data:\n",
    "# check format of whole file\n",
    "#!cat anonymous-msweb.data\n",
    "write_file_loc = \"msweb_processed.txt\"\n",
    "writefile = open(write_file_loc, 'wb')\n",
    "\n",
    "with open(\"anonymous-msweb.data\", 'rU') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(',')\n",
    "        \n",
    "        # skip all the header rows\n",
    "        if line[0] not in ['C', 'V']:\n",
    "            continue\n",
    "            \n",
    "        if line[0] == 'C':\n",
    "            user_id = line[2]\n",
    "        \n",
    "        elif line[0] == 'V':\n",
    "            writefile.write('V,{},1,C,{}\\n'.format(line[1], user_id))\n",
    "        \n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,1,C,10001\r\n",
      "V,1001,1,C,10001\r\n",
      "V,1002,1,C,10001\r\n",
      "V,1001,1,C,10002\r\n",
      "V,1003,1,C,10002\r\n",
      "V,1001,1,C,10003\r\n",
      "V,1003,1,C,10003\r\n",
      "V,1004,1,C,10003\r\n",
      "V,1005,1,C,10004\r\n",
      "V,1006,1,C,10005\r\n"
     ]
    }
   ],
   "source": [
    "# preprocessed file is:\n",
    "!head -n10 msweb_processed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I,4,\"www.microsoft.com\",\"created by getlog.pl\"\r\n",
      "T,1,\"VRoot\",0,0,\"VRoot\"\r\n",
      "N,0,\"0\"\r\n",
      "N,1,\"1\"\r\n",
      "T,2,\"Hide1\",0,0,\"Hide\"\r\n",
      "N,0,\"0\"\r\n",
      "N,1,\"1\"\r\n",
      "A,1287,1,\"International AutoRoute\",\"/autoroute\"\r\n",
      "A,1288,1,\"library\",\"/library\"\r\n",
      "A,1289,1,\"Master Chef Product Information\",\"/masterchef\"\r\n",
      "A,1297,1,\"Central America\",\"/centroam\"\r\n",
      "A,1215,1,\"For Developers Only Info\",\"/developer\"\r\n",
      "A,1279,1,\"Multimedia Golf\",\"/msgolf\"\r\n",
      "A,1239,1,\"Microsoft Consulting\",\"/msconsult\"\r\n",
      "A,1282,1,\"home\",\"/home\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -n15 anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-02-15 15:20:24--  http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/anonymous-msweb.data\n",
      "Resolving archive.ics.uci.edu... 128.195.10.249\n",
      "Connecting to archive.ics.uci.edu|128.195.10.249|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1423098 (1.4M) [text/plain]\n",
      "Saving to: 'anonymous-msweb.data'\n",
      "\n",
      "anonymous-msweb.dat 100%[=====================>]   1.36M  1.88MB/s   in 0.7s   \n",
      "\n",
      "2016-02-15 15:20:25 (1.88 MB/s) - 'anonymous-msweb.data' saved [1423098/1423098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## now process a new set of data with just URL and webpageID for merging\n",
    "## first get 4.2 data and preprocess it:\n",
    "!wget -O anonymous-msweb.data http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/anonymous-msweb.data\n",
    "\n",
    "## python code to reformat the data:\n",
    "# check format of whole file\n",
    "#!cat anonymous-msweb.data\n",
    "write_file_loc = \"msweb_dim_url.txt\"\n",
    "writefile = open(write_file_loc, 'wb')\n",
    "\n",
    "with open(\"anonymous-msweb.data\", 'rU') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split(',')\n",
    "        \n",
    "        # skip all the header rows\n",
    "        if line[0] == 'A':\n",
    "            # attach the relative url to the home url. Remove \"\"'s\n",
    "            full_url = \"http://www.microsoft.com\"+line[4].strip('\"')\n",
    "            writefile.write('{url_id}\\t{url}\\n'.format(url_id=line[1],url=full_url))\n",
    "        \n",
    "        #elif line[0] == 'V':\n",
    "        #    writefile.write('V,{},1,C,{}\\n'.format(line[1], user_id))\n",
    "        \n",
    "        #else:\n",
    "        #    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287\thttp://www.microsoft.com/autoroute\r\n",
      "1288\thttp://www.microsoft.com/library\r\n",
      "1289\thttp://www.microsoft.com/masterchef\r\n",
      "1297\thttp://www.microsoft.com/centroam\r\n",
      "1215\thttp://www.microsoft.com/developer\r\n",
      "1279\thttp://www.microsoft.com/msgolf\r\n",
      "1239\thttp://www.microsoft.com/msconsult\r\n",
      "1282\thttp://www.microsoft.com/home\r\n",
      "1251\thttp://www.microsoft.com/referencesupport\r\n",
      "1121\thttp://www.microsoft.com/magazine\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10 msweb_dim_url.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     304 msweb_dim_url.txt\n",
      "   98654 msweb_processed.txt\n"
     ]
    }
   ],
   "source": [
    "!wc -l msweb_dim_url.txt\n",
    "!wc -l msweb_processed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MR_HW52_joins.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MR_HW52_joins.py\n",
    "#!/usr/bin/env python\n",
    "# find the most frequent visitors per url\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from heapq import heappush, heappop,nlargest\n",
    "\n",
    "class MapSideJoins(MRJob):\n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "            mapper_init = self.mapper_init,\n",
    "            mapper = self.mapper,\n",
    "            mapper_final = self.mapper_final\n",
    "            )]\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        # set variables\n",
    "        # inspired by example on lecture slide 121\n",
    "        self.URLS = {}\n",
    "        \n",
    "        # for right outer join\n",
    "        self.URLS_ON_RIGHT = {}\n",
    "        \n",
    "        # initialize row counts\n",
    "        self.left = 0\n",
    "        self.right = 0 \n",
    "        self.inner = 0\n",
    "        \n",
    "        # read in the page_url table\n",
    "        # formatted like : 1287 http://www.microsoft.com/autoroute\n",
    "        f = open(\"msweb_dim_url.txt\", 'rU')\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            data = line.split()\n",
    "            #store page-id as key in URLS dictionary\n",
    "            self.URLS[data[0]] = data[1]\n",
    "                \n",
    "    def mapper(self, _, line):\n",
    "        \"\"\"stream in output from 4.2 (msweb_processed.txt):\n",
    "        \n",
    "        V,1000,1,C,10001\n",
    "        \n",
    "        and match with keys in self.URLS\"\"\"\n",
    "        self.increment_counter('group', 'Number_mapper_calls',1)\n",
    "        line = line.strip().split(',')\n",
    "        key = line[1]\n",
    "        \n",
    "        if key in self.URLS.keys():\n",
    "            ## outputs are : JOIN_TYPE, URL_ID, URL, CUSTOMER_ID\n",
    "            # map inner join\n",
    "            self.inner += 1\n",
    "            yield None, (\"inner\", key, self.URLS[key], line[4])\n",
    "            \n",
    "            # map visits left join urls on url_id\n",
    "            # this code assumes each customer has a visit\n",
    "            # if there are visits to no URLS, we will catch it in the second mapper\n",
    "            # although this probably isn't the case...\n",
    "            self.left += 1\n",
    "            yield None, (\"left\", key, self.URLS[key], line[4])\n",
    "            \n",
    "            # map visits right join urls on url_id\n",
    "            # keep track of url_ids that have not been visited\n",
    "            self.right += 1\n",
    "            yield None, (\"right\", key, self.URLS[key], line[4])\n",
    "            self.URLS_ON_RIGHT[key] = self.URLS[key]\n",
    "        \n",
    "        #right join will also include sites that have not been visited\n",
    "        # no customer_id\n",
    "        else:\n",
    "            self.right += 1\n",
    "            yield None, (\"right\", key, self.URLS[key], \"NA\")\n",
    "            \n",
    "    def mapper_final(self):\n",
    "        for key in self.URLS.keys():\n",
    "            # in a left join, if a key exists on the left table, but not the right table\n",
    "            # then we need a row with data from the left table and null values from the missing\n",
    "            # data on the right\n",
    "            if key not in self.URLS_ON_RIGHT.keys():\n",
    "                self.left += 1\n",
    "                yield None, (\"left\", key, \"NA\",  self.URLS[key])\n",
    "        \n",
    "        ## final outputs:\n",
    "        yield None, ()\n",
    "        yield None, (\"left-joined rows: \" + str(self.left))\n",
    "        yield None, (\"right-joined rows: \" + str(self.right))\n",
    "        yield None, (\"inner-joined rows: \" + str(self.inner))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    MapSideJoins.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/jz/dhc2gzfj2091nhfmrbvv3cjh0000gn/T/MR_HW52_joins.Safyre.20160216.091239.013019\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/jz/dhc2gzfj2091nhfmrbvv3cjh0000gn/T/MR_HW52_joins.Safyre.20160216.091239.013019/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  group:\n",
      "    Number_mapper_calls: 98654\n",
      "Moving /var/folders/jz/dhc2gzfj2091nhfmrbvv3cjh0000gn/T/MR_HW52_joins.Safyre.20160216.091239.013019/step-0-mapper_part-00000 -> /var/folders/jz/dhc2gzfj2091nhfmrbvv3cjh0000gn/T/MR_HW52_joins.Safyre.20160216.091239.013019/output/part-00000\n",
      "Streaming final output from /var/folders/jz/dhc2gzfj2091nhfmrbvv3cjh0000gn/T/MR_HW52_joins.Safyre.20160216.091239.013019/output\n",
      "removing tmp directory /var/folders/jz/dhc2gzfj2091nhfmrbvv3cjh0000gn/T/MR_HW52_joins.Safyre.20160216.091239.013019\n"
     ]
    }
   ],
   "source": [
    "!rm hw52_out.txt\n",
    "!./MR_HW52_joins.py msweb_processed.txt \\\n",
    "    --file msweb_dim_url.txt > hw52_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\t[\"left\", \"1291\", \"NA\", \"http://www.microsoft.com/news\"]\r\n",
      "null\t[\"left\", \"1297\", \"NA\", \"http://www.microsoft.com/centroam\"]\r\n",
      "null\t[\"left\", \"1294\", \"NA\", \"http://www.microsoft.com/bookshelf\"]\r\n",
      "null\t[\"left\", \"1287\", \"NA\", \"http://www.microsoft.com/autoroute\"]\r\n",
      "null\t[\"left\", \"1289\", \"NA\", \"http://www.microsoft.com/masterchef\"]\r\n",
      "null\t[\"left\", \"1288\", \"NA\", \"http://www.microsoft.com/library\"]\r\n",
      "null\t[]\r\n",
      "null\t\"left-joined rows: 98663\"\r\n",
      "null\t\"right-joined rows: 98654\"\r\n",
      "null\t\"inner-joined rows: 98654\"\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n10 hw52_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got unexpected keyword arguments: ssh_tunnel\n",
      "using existing scratch bucket mrjob-62fd57571f35a64e\n",
      "using s3://mrjob-62fd57571f35a64e/tmp/ as our scratch dir on S3\n",
      "creating tmp directory /var/folders/jz/dhc2gzfj2091nhfmrbvv3cjh0000gn/T/MR_HW52_joins.Safyre.20160216.092228.655665\n",
      "writing master bootstrap script to /var/folders/jz/dhc2gzfj2091nhfmrbvv3cjh0000gn/T/MR_HW52_joins.Safyre.20160216.092228.655665/b.py\n",
      "Copying non-input files into s3://mrjob-62fd57571f35a64e/tmp/MR_HW52_joins.Safyre.20160216.092228.655665/files/\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Creating Elastic MapReduce job flow\n",
      "Job flow created with ID: j-1TOO3XZPEKZ4R\n",
      "Created new job flow j-1TOO3XZPEKZ4R\n",
      "Job launched 30.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 61.5s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 92.0s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 122.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 153.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 183.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 214.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 244.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 275.4s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 306.1s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 336.6s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 367.2s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 397.7s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 428.3s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 458.9s ago, status STARTING: Provisioning Amazon EC2 capacity\n",
      "Job launched 489.5s ago, status STARTING: Configuring cluster software\n",
      "Job launched 520.1s ago, status STARTING: Configuring cluster software\n",
      "Job launched 550.7s ago, status STARTING: Configuring cluster software\n",
      "Job launched 581.2s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 611.8s ago, status BOOTSTRAPPING: Running bootstrap actions\n",
      "Job launched 642.3s ago, status RUNNING: Running step\n",
      "Job on job flow j-1TOO3XZPEKZ4R failed with status TERMINATING: Shut down as step failed\n",
      "Logs are in s3://mrjob-62fd57571f35a64e/tmp/logs/j-1TOO3XZPEKZ4R/\n",
      "Scanning S3 logs for probable cause of failure\n",
      "Waiting 5.0s for S3 eventual consistency\n",
      "Terminating job flow: j-1TOO3XZPEKZ4R\n",
      "Traceback (most recent call last):\n",
      "  File \"MR_HW52_joins.py\", line 89, in <module>\n",
      "    MapSideJoins.run()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/job.py\", line 461, in run\n",
      "    mr_job.execute()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/job.py\", line 479, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/launch.py\", line 153, in execute\n",
      "    self.run_job()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/launch.py\", line 216, in run_job\n",
      "    runner.run()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/runner.py\", line 470, in run\n",
      "    self._run()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/emr.py\", line 883, in _run\n",
      "    self._wait_for_job_to_complete()\n",
      "  File \"/Library/Python/2.7/site-packages/mrjob/emr.py\", line 1768, in _wait_for_job_to_complete\n",
      "    raise Exception(msg)\n",
      "Exception: Job on job flow j-1TOO3XZPEKZ4R failed with status TERMINATING: Shut down as step failed\n"
     ]
    }
   ],
   "source": [
    "!chmod +x MR_HW52_joins.py\n",
    "!python MR_HW52_joins.py -r emr s3://w261-safyre-hw52-input/msweb_processed.txt \\\n",
    "    --conf-path /Users/Safyre/cc-mrjob/mrjob.conf\\\n",
    "    --file s3://w261-safyre-hw52-input/msweb_dim_url.txt \\\n",
    "    --output-dir= s3://w261-safyre-hw52-output/ \\\n",
    "    --no-output \\\n",
    "    --cleanup=NONE\\\n",
    "    --no-strict-protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5.3  EDA of Google n-grams dataset\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "   https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "   `s3://filtered-5grams/`\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating  your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-02-15 23:42:40--  https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACr50woxiBWoaiiLmnwduX8a/googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0\n",
      "Resolving www.dropbox.com... 108.160.172.206, 108.160.172.238\n",
      "Connecting to www.dropbox.com|108.160.172.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://dl.dropboxusercontent.com/content_link/aIZVHfoBfzACzWxyagqbBmxeGTl601USgFACattc1MpkSXDxZ9gk5hMyZTQFqWBn/file [following]\n",
      "--2016-02-15 23:42:41--  https://dl.dropboxusercontent.com/content_link/aIZVHfoBfzACzWxyagqbBmxeGTl601USgFACattc1MpkSXDxZ9gk5hMyZTQFqWBn/file\n",
      "Resolving dl.dropboxusercontent.com... 199.47.217.69\n",
      "Connecting to dl.dropboxusercontent.com|199.47.217.69|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11444614 (11M) [text/plain]\n",
      "Saving to: 'google-5grams-test.txt'\n",
      "\n",
      "google-5grams-test. 100%[=====================>]  10.91M  1.03MB/s   in 8.3s   \n",
      "\n",
      "2016-02-15 23:42:50 (1.31 MB/s) - 'google-5grams-test.txt' saved [11444614/11444614]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## download dev data\n",
    "!wget -O google-5grams-test.txt \"https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACr50woxiBWoaiiLmnwduX8a/googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A BILL FOR ESTABLISHING RELIGIOUS\t59\t59\t54\r\n",
      "A Biography of General George\t92\t90\t74\r\n",
      "A Case Study in Government\t102\t102\t78\r\n",
      "A Case Study of Female\t447\t447\t327\r\n",
      "A Case Study of Limited\t55\t55\t43\r\n",
      "A Child's Christmas in Wales\t1099\t1061\t866\r\n",
      "A Circumstantial Narrative of the\t62\t62\t50\r\n",
      "A City by the Sea\t62\t60\t49\r\n",
      "A Collection of Fairy Tales\t123\t117\t80\r\n",
      "A Collection of Forms of\t116\t103\t82\r\n"
     ]
    }
   ],
   "source": [
    "#ngram \\t count \\t page_count \\t book_count\n",
    "!head -n10 google-5grams-test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11444614"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## upload test set to s3 bucket\n",
    "import boto\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "\n",
    "# AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are environment variables\n",
    "# S3Connection argument can be empty\n",
    "conn = S3Connection()\n",
    "bucket = conn.create_bucket('w261-safyre-hw53-input')  # sub-datasets bucket already exists\n",
    "myBucket = conn.get_bucket('w261-safyre-hw53-input')\n",
    "\n",
    "k = Key(myBucket)\n",
    "k.key = 'google-5grams-test.txt'\n",
    "k.set_contents_from_filename('google-5grams-test.txt')\n",
    "#googlebooks-eng-all-5gram-20090715-0-filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapper for 5.3 word count\n",
    "%%writefile MR_HW53b.py\n",
    "#!/usr/bin/env python\n",
    "# find the most frequent visitors per url\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from heapq import heappush, heappop,nlargest\n",
    "import re\n",
    "\n",
    "class WordCount(MRJob):\n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "            mapper_init = self.mapper_init,\n",
    "            mapper = self.mapper,\n",
    "            combiner = self.combiner,\n",
    "            reducer = self.reducer\n",
    "            ),\n",
    "            MRStep(\n",
    "            reducer = self.reducer_2nd\n",
    "            )]\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        self.counts = {}\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        [ngram, gram_count,pages,books] = re.split(\"\\t\",line)\n",
    "        gram_count = int(gram_count)\n",
    "        words = re.split(\" \",ngram)\n",
    "        for word in words:\n",
    "            self.counts.setdefault(word,0)\n",
    "            self.counts[word] += gram_count\n",
    "        for word in counts.keys():\n",
    "            yield word, counts[word]\n",
    "                \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, total):\n",
    "        total = sum(counts)\n",
    "        yield None,(word, total)\n",
    "        \n",
    "    def reducer_2nd(self, _, word_count_pairs):\n",
    "        heap = []\n",
    "        for p in word_count_pairs:\n",
    "            heappush(heap,p)\n",
    "        top10 = nlargest(10, heap)\n",
    "        yield None,top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x MR_HW53b.py\n",
    "!./MR_HW53b.py google-5grams-test.txt > hw53b_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapper for 5.3 ngrams EDA\n",
    "%%writefile MR_HW53acd.py\n",
    "#!/usr/bin/env python\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import csv\n",
    "from heapq import heappush, heappop,nlargest\n",
    "import re\n",
    "\n",
    "class WordCount(MRJob):\n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "            mapper_init = self.mapper_init,\n",
    "            mapper = self.mapper,\n",
    "            mapper_final = self.mapper_final\n",
    "            )]\n",
    "        \n",
    "    def mapper_init(self):\n",
    "        self.counts = {}\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line.strip()\n",
    "        [ngram, gram_count,pages,books] = re.split(\"\\t\",line)\n",
    "        gram_count = int(gram_count)\n",
    "        words = re.split(\" \",ngram)\n",
    "        for word in words:\n",
    "            self.counts.setdefault(word,0)\n",
    "            self.counts[word] += gram_count\n",
    "        for word in counts.keys():\n",
    "            yield word, counts[word]\n",
    "                \n",
    "    def combiner(self, word, counts):\n",
    "        total = sum(counts)\n",
    "        yield word, total\n",
    "    \n",
    "    def reducer(self, word, total):\n",
    "        total = sum(counts)\n",
    "        yield None,(word, total)\n",
    "        \n",
    "    def reducer_2nd(self, _, word_count_pairs):\n",
    "        heap = []\n",
    "        for p in word_count_pairs:\n",
    "            heappush(heap,p)\n",
    "        top10 = nlargest(10, heap)\n",
    "        yield None,top10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
