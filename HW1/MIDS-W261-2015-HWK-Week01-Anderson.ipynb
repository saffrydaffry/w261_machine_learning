{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science W261: Machine Learning at Scale\n",
    "**Safyre Anderson**\n",
    "\n",
    "**safyre@berkeley.edu**\n",
    "\n",
    "**January 21, 2016, 8am**\n",
    "\n",
    "**W261-3**\n",
    "\n",
    "**Week 1 HW**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise.*\n",
    "\n",
    "Big data refers to the problem of storage and analysis of datasets that overwhelm a traditional single computer system, and typically characterized by the \"3 V's\": volume, velocity, and veracity. The extent of either one of these 3 V's (the size of the dataset, the speed of ingestion, and the variety of features) prevents Big Data from being analyzed on the limited hardware of a personal computer or laptop. Currently, I work in FinTech and one of the most important Big Data problems that exists is the near real-time detection of fraudulent transactions. There are over $1 billion people online that transact between 400 billion to 1 billion times per day (depending on what article you read), with variability in customers, transfers, purchases, payment methods, and many more kinds of recorded data--easily fulfilling the 3 V's. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HW1.0.1.In 500 words (English or pseudo code or a combination) \n",
    "describe how to estimate the bias, the variance, the irreduciable error for a test dataset T \n",
    "when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?*\n",
    "\n",
    "One of the recommended readings provided an excellent framework for this problem: https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/.\n",
    "\n",
    "Essentially, total error of a model is the sum of the squared error of bias, variance, and residual squared (noise). Thus on a high level, we will want to to determine the bias, variance, and total error of each model we test. From these three parameters, we can estimate the noise of each estimated true model by subtracting the mean squared error  from the total test error:\n",
    "\n",
    "Expected Prediction Error $= E[(g(x^*) - E[g(x^*)]^2] + (E[g(x^*)] - f(x^*))^2 + E[(y^*-f(x))^2] = E[(g(x^*) - y^*)^2]$\n",
    "\n",
    "In our case, $f(x)$ is unknown. However, we are given $y^*$ and we can bootstrap training and test data to get $g(x^*)$ and E[g(x^* )]$ over all models (of the same degree).\n",
    "\n",
    "In general:\n",
    "\n",
    "Given a dataset:\n",
    "\n",
    "` for each polynomial of degree 1 through 5:`\n",
    "    \n",
    "    for each bootstrapped sample of the dataset:\n",
    "        subset $x%$ to training\n",
    "        subset $1-x%$ to testing, where x is a fraction of the whole dataset\n",
    "        \n",
    "        fit a model on the training data\n",
    "        predict $g(x^*)$ on the testing data\n",
    "        \n",
    "        calculate estimated predicted error (mean squared difference of observed $y^*_{test}$ from predicted y ($g(x^*)$).\n",
    "        calculate variance (mean of the variance of $g(x^*)$ across all datasets)\n",
    "\n",
    "\n",
    "Here, we should already by able to select the best model by choosing the parameter(s) that achieved the lowest estimated predicted error. What about bias and noise? We still haven't figured those out yet.\n",
    "\n",
    "Since noise is independent of the model, it is a constant term in the above equation. Thus, we can rethink this problem as such: $Expected~Error - Variance = Bias + Constant$. To estimate the noise, we can take advantage of the bias variance tradeoff-- with the highest complexity model, we can assume bias approaches 0.  Thus the equation becomes $ Expected~Error - Variance = Constant$. At polynomial N = 5, we can estimate the noise and finally solve for the bias: $Bias = Expected~Error - Variance - Constant$.\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "In the remainder of this assignment you will produce a spam filter\n",
    "that is backed by a multinomial naive Bayes classifier b (see http://nlp.stanford.edu/IR-book/html/htmledition/properties-of-naive-bayes-1.html),\n",
    "which counts words in parallel via \n",
    "a unix, poor-man's map-reduce framework.\n",
    "\n",
    "For the sake of this assignment\n",
    "we will focus on the basic construction \n",
    "of the parallelized classifier,\n",
    "and not consider its validation or calibration,\n",
    "and so you will have the classifier operate\n",
    "on its own training data (unlike a field application where one would use non-overlapping subsets for training, validation and testing).\n",
    "\n",
    "The data you will use is a curated subset of the Enron email corpus\n",
    "(whose details you may find in the file enronemail_README.txt \n",
    "in the directory surrounding these instructions).\n",
    "\n",
    "=====Instructions/Goals=====\n",
    "\n",
    "In this directory you will also find starter code (pNaiveBayes.sh),\n",
    "(similar to the pGrepCount.sh code that was presented in this weeks lectures),\n",
    "which will be used as control script to a python mapper and reducer \n",
    "that you will supply at several stages. Doing some exploratory data analysis you will see (with this very small dataset) the following\\:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "` > wc -l enronemail_1h.txt  #100 email records`\n",
    "\n",
    "      100 enronemail_1h.txt\n",
    "     \n",
    "` > cut -f2 -d$'\\t' enronemail_1h.txt|wc  #extract second field which is SPAM flag`\n",
    "\n",
    "      101     394    3999\n",
    "     \n",
    "`JAMES-SHANAHANs-Desktop-Pro-2:HW1-Questions jshanahan\\$ cut -f2 -d$'\\t' enronemail_1h.txt|head`\n",
    "\n",
    "0\n",
    "\n",
    "0\n",
    "\n",
    "0\n",
    "\n",
    "0\n",
    "\n",
    "0\n",
    "\n",
    "0\n",
    "\n",
    "0\n",
    "\n",
    "0\n",
    "\n",
    "1\n",
    "\n",
    "1\n",
    "\n",
    "` > head -n 100 enronemail_1h.txt|tail -1|less #an example SPAM email record`\n",
    "\n",
    "` 018.2001-07-13.SA_and_HP       1  `      \n",
    "\n",
    "`[ilug] we need your assistance to invest in your country        dear sir/madam,  i am well confident of your capability to assist me in  a transaction for mutual benefit of both parties, ie  (me and you) i am also believing that you will not  expose or betray the trust and confidence i am about  to establish with you. i have decided to contact you  with greatest delight and personal respect.  well, i am victor sankoh, son to \n",
    "mr. foday  sankoh  who was arrested by the ecomog peace keeping force  months ago in my country sierra leone.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "and all of its comments. When you are comfortable with their\n",
    "purpose and function, respond to the remaining homework questions below. \n",
    "A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(standard_in) 1: parse error\n",
      "split: enronemail_1h.txt: illegal line count\n",
      "Traceback (most recent call last):\n",
      "  File \"./reducer.py\", line 27, in <module>\n",
      "    spam_total += int(spam_count)\n",
      "NameError: name 'spam_count' is not defined\n"
     ]
    }
   ],
   "source": [
    "# just to run it to show i've read it. I know there aren't mappers and reducers yet!\n",
    "!chmod +x pNaiveBayes.sh\n",
    "!./pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "\n",
    "   To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word.\n",
    "\n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l\n",
    "       8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-01-22 07:36:56--  https://www.dropbox.com/sh/jylzkmauxkostck/AAC_6JZH7yqMcxfEGPc4-_xJa/enronemail_1h.txt?dl=0\n",
      "Resolving www.dropbox.com... 108.160.172.238, 108.160.172.206\n",
      "Connecting to www.dropbox.com|108.160.172.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://dl.dropboxusercontent.com/content_link/UPXdQChI66QKyyBxhl07rNOngofwByuL0wSayFJjt1CrDsxj34imSfJI6bJFxMTZ/file [following]\n",
      "--2016-01-22 07:36:59--  https://dl.dropboxusercontent.com/content_link/UPXdQChI66QKyyBxhl07rNOngofwByuL0wSayFJjt1CrDsxj34imSfJI6bJFxMTZ/file\n",
      "Resolving dl.dropboxusercontent.com... 199.47.217.5\n",
      "Connecting to dl.dropboxusercontent.com|199.47.217.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 204579 (200K) [text/plain]\n",
      "Saving to: 'enronemail_1h.txt'\n",
      "\n",
      "enronemail_1h.txt   100%[=====================>] 199.78K   399KB/s   in 0.5s   \n",
      "\n",
      "2016-01-22 07:37:01 (399 KB/s) - 'enronemail_1h.txt' saved [204579/204579]\n",
      "\n",
      "      99 enronemail_1h.txt\n"
     ]
    }
   ],
   "source": [
    "# download into current directory\n",
    "!wget https://www.dropbox.com/sh/jylzkmauxkostck/AAC_6JZH7yqMcxfEGPc4-_xJa/enronemail_1h.txt?dl=0 -O enronemail_1h.txt\n",
    "    \n",
    "# running wc -l enronemail_1h.txt outputs linecount = 0\n",
    "# due to \\r line break, need to change to \\n line break (mac, unix, respectively)\n",
    "!perl -pi -e 's/\\r/\\n/g' enronemail_1h.txt\n",
    "\n",
    "!wc -l enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# store a regex expression into a pattern object\n",
    "# that seeks words including underscores and single quotes\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "# file input\n",
    "filename = sys.argv[1]\n",
    "\n",
    "# for this part, just assume word_list is length 1\n",
    "word_list = sys.argv[2]\n",
    "count = 0\n",
    "\n",
    "with open(filename, 'rU') as f:\n",
    "    for line in f.readlines():\n",
    "        for word in word_list.split():\n",
    "            counts = [int(1) if x == word else int(0) for x in WORD_RE.findall(line)]\n",
    "            counts = np.array(counts)\n",
    "            \n",
    "            if counts.sum() > 0:\n",
    "                print word + \" \" + str(counts.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the mapper.py an executable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "files_list = sys.argv[1].split()\n",
    "\n",
    "cur_word = \"\"\n",
    "\n",
    "word_counts = {}\n",
    "\n",
    "for f in files_list:\n",
    "    with open(f, 'rU') as countfile:\n",
    "        for line in countfile.readlines():\n",
    "            new_word, count = line.split()\n",
    "            if new_word != cur_word:\n",
    "                # print the final count of the current word and start over\n",
    "                cur_word = new_word\n",
    "                word_counts[cur_word] = word_counts.get(cur_word, 0) + int(count)\n",
    "        \n",
    "            else:\n",
    "                word_counts[new_word] += int(count)\n",
    "    \n",
    "for word in word_counts:\n",
    "    print word + \"\\t\" + str(word_counts[word])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "#!/bin/bash\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py \"$countfiles\" > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run for 1.2, first argument is number of mappers, second argument is a list of words. File list was printed only for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!head enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm answers with bash commands (note there are actually 10 instances of assistance, but only 8 lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       8\r\n"
     ]
    }
   ],
   "source": [
    "!grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "\n",
    "   that performs a single word Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# store a regex expression into a pattern object\n",
    "# that seeks words including underscores and single quotes\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "TRUTH_RE = re.compile(r\"\\t(\\d)\\t\")\n",
    "\n",
    "# file input\n",
    "filename = sys.argv[1]\n",
    "\n",
    "# for this part, just assume word_list is length 1\n",
    "word_list = sys.argv[2]\n",
    "\n",
    "# Avoid KeyError if no data in chunk\n",
    "#counts_dict = dict.fromkeys(['0', '1'], 0)\n",
    "counts_dict = {}\n",
    "doc_len    = 0\n",
    "spam_count = 0\n",
    "ham_count  = 0\n",
    "\n",
    "with open(filename, 'rU') as f:\n",
    "    for line in f.readlines():\n",
    "        # Remove punctuation\n",
    "        line = re.sub(r'[^\\w\\s]','',line)\n",
    "        \n",
    "        # truth is the actual label provided in the data\n",
    "        # 1 = spam, 0 = ham\n",
    "        truth = TRUTH_RE.findall(line)[0]\n",
    "        \n",
    "        '''\n",
    "        if truth == '1':\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            ham_count += 1\n",
    "        '''\n",
    "        for category in ['0','1']:\n",
    "            counts_dict[category] = {}\n",
    "        \n",
    "        for word in word_list.split():\n",
    "            doc_len = len(word_list.split())\n",
    "            counts = [1 if x == word else 0 for x in WORD_RE.findall(line)]\n",
    "            counts = np.array(counts)  \n",
    "\n",
    "            if counts.sum() > 0:\n",
    "                count = counts.sum()\n",
    "                counts_dict[truth][word] = counts_dict[truth].get(word, 0) + int(count)\n",
    "            else:\n",
    "                counts_dict[truth][word] = 0\n",
    "               \n",
    "\n",
    "for category, word_dictionary in counts_dict.iteritems():\n",
    "    for words, count in counts_dict[category].iteritems():\n",
    "        print category + \"\\t\" + words + \"\\t\" + str(count) + \"\\t\" + str(doc_len)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "TRUTH_RE = re.compile(r\"\\t(\\d)\\t\")\n",
    "files_list = sys.argv[1].split()\n",
    "\n",
    "## training, gather all the counts and calculate corpus-wide priors, etc\n",
    "## data come in as strings, \n",
    "## TRUTH WORD COUNT SPAM_COUNT HAM_COUNT\n",
    "counts_dict = {}\n",
    "for category in ['0','1']:\n",
    "    counts_dict[category] = {}\n",
    "\n",
    "spam_total = 0\n",
    "ham_total  = 0\n",
    "\n",
    "for f in files_list:\n",
    "    with open(f, 'rU') as countfile:\n",
    "        for line in countfile.readlines():\n",
    "            truth, word, count, doc_len = line.split()\n",
    "            counts_dict[truth][word] = counts_dict[truth].get(word, 0) + int(count)\n",
    "            if truth == '1':\n",
    "                spam_total += int(doc_len)\n",
    "            else:\n",
    "                ham_total  += int(doc_len)\n",
    "        \n",
    "priors = {'0': float(ham_total)/(spam_total+ham_total),\n",
    "          '1': float(spam_total)/(spam_total+ham_total)}\n",
    "\n",
    "prior_counts = {'0': float(ham_total),\n",
    "          '1': float(spam_total)}\n",
    "\n",
    "print \"Priors are: \"\n",
    "for category in priors:\n",
    "    print category + \" \" + str(priors[category]) + \"\\n\"\n",
    "\n",
    "spam_vocab = counts_dict['1'].keys()\n",
    "ham_vocab  = counts_dict['0'].keys()\n",
    "\n",
    "spam_vocab_n = len(counts_dict['1'].keys())\n",
    "ham_vocab_n  = len(counts_dict['0'].keys())\n",
    "\n",
    "## although we are not implementing Laplace Transform\n",
    "# probably don't need these next two lines\n",
    "#vocab = union(spam_vocab, ham_vocab)\n",
    "#vocab_n = len(vocab)\n",
    "\n",
    "## Calculate conditional probabilities\n",
    "## P(word | class) \n",
    "posteriors = {}\n",
    "for category in ['0', '1']:\n",
    "    posteriors[category] = {}\n",
    "    for word in counts_dict[category].keys():\n",
    "        posteriors[category][word] = float(counts_dict[category][word])/prior_counts[category]\n",
    "\n",
    "print \"\\nPosteriors are: \"\n",
    "for category in posteriors:\n",
    "    for word in posteriors[category]:\n",
    "        print word + \" in class \" + category + \" \" + str(posteriors[category][word]) + \"\\n\"\n",
    "\n",
    "## Testing the classifer\n",
    "## Without laplacian transform \n",
    "print \"DOC_ID | TRUTH | CLASS \"\n",
    "print \"=======================\\n\"\n",
    "\n",
    "doc_id = 0\n",
    "correct = 0\n",
    "with open(\"enronemail_1h.txt\", 'rU') as testdata:\n",
    "    for line in testdata.readlines():\n",
    "        score = [0,0]\n",
    "        line = re.sub(r'[^\\w\\s]','',line)\n",
    "        truth = TRUTH_RE.findall(line)[0]\n",
    "        \n",
    "        for category in ['0', '1']:\n",
    "            idx = int(category)\n",
    "            score[idx] =  priors[category]\n",
    "            for word in posteriors[category]:\n",
    "                if word in WORD_RE.findall(line):\n",
    "                    score[idx] *= float(posteriors[category][word])\n",
    "                #print \"\\n\", idx, score[idx], category, word\n",
    "        score = np.array(score)\n",
    "        prediction = score.argmax()\n",
    "        doc_id +=1\n",
    "        \n",
    "        if int(prediction) == int(truth):\n",
    "            correct +=1\n",
    "        print str(doc_id) + \"\\t\" + truth + \"\\t\" +str(prediction) +  \" \" +str(score[0]) + \" \" + str(score[1])\n",
    "\n",
    "accuracy = float(correct)/doc_id*100.0\n",
    "print \"Accuracy: \", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors are: \r\n",
      "1 0.5\r\n",
      "\r\n",
      "0 0.5\r\n",
      "\r\n",
      "\r\n",
      "Posteriors are: \r\n",
      "assistance in class 1 0.5\r\n",
      "\r\n",
      "assistance in class 0 0.0\r\n",
      "\r\n",
      "DOC_ID | TRUTH | CLASS \r\n",
      "=======================\r\n",
      "\r\n",
      "1\t0\t0 0.5 0.5\r\n",
      "2\t0\t0 0.5 0.5\r\n",
      "3\t0\t0 0.5 0.5\r\n",
      "4\t0\t0 0.5 0.5\r\n",
      "5\t0\t0 0.5 0.5\r\n",
      "6\t0\t0 0.5 0.5\r\n",
      "7\t0\t0 0.5 0.5\r\n",
      "8\t0\t0 0.5 0.5\r\n",
      "9\t1\t0 0.5 0.5\r\n",
      "10\t1\t0 0.5 0.5\r\n",
      "11\t1\t1 0.0 0.25\r\n",
      "12\t0\t0 0.5 0.5\r\n",
      "13\t0\t0 0.5 0.5\r\n",
      "14\t0\t0 0.5 0.5\r\n",
      "15\t0\t0 0.5 0.5\r\n",
      "16\t1\t0 0.5 0.5\r\n",
      "17\t1\t0 0.5 0.5\r\n",
      "18\t0\t1 0.0 0.25\r\n",
      "19\t0\t0 0.5 0.5\r\n",
      "20\t0\t0 0.5 0.5\r\n",
      "21\t1\t0 0.5 0.5\r\n",
      "22\t1\t0 0.5 0.5\r\n",
      "23\t0\t1 0.0 0.25\r\n",
      "24\t0\t0 0.5 0.5\r\n",
      "25\t0\t0 0.5 0.5\r\n",
      "26\t0\t0 0.5 0.5\r\n",
      "27\t1\t0 0.5 0.5\r\n",
      "28\t1\t0 0.5 0.5\r\n",
      "29\t0\t0 0.5 0.5\r\n",
      "30\t0\t0 0.5 0.5\r\n",
      "31\t0\t0 0.5 0.5\r\n",
      "32\t1\t0 0.5 0.5\r\n",
      "33\t1\t0 0.5 0.5\r\n",
      "34\t1\t0 0.5 0.5\r\n",
      "35\t0\t0 0.5 0.5\r\n",
      "36\t0\t0 0.5 0.5\r\n",
      "37\t0\t0 0.5 0.5\r\n",
      "38\t0\t0 0.5 0.5\r\n",
      "39\t1\t0 0.5 0.5\r\n",
      "40\t1\t0 0.5 0.5\r\n",
      "41\t0\t0 0.5 0.5\r\n",
      "42\t1\t0 0.5 0.5\r\n",
      "43\t1\t0 0.5 0.5\r\n",
      "44\t1\t0 0.5 0.5\r\n",
      "45\t1\t0 0.5 0.5\r\n",
      "46\t0\t0 0.5 0.5\r\n",
      "47\t0\t0 0.5 0.5\r\n",
      "48\t0\t0 0.5 0.5\r\n",
      "49\t0\t0 0.5 0.5\r\n",
      "50\t1\t0 0.5 0.5\r\n",
      "51\t1\t0 0.5 0.5\r\n",
      "52\t0\t0 0.5 0.5\r\n",
      "53\t0\t0 0.5 0.5\r\n",
      "54\t0\t0 0.5 0.5\r\n",
      "55\t1\t1 0.0 0.25\r\n",
      "56\t1\t0 0.5 0.5\r\n",
      "57\t1\t0 0.5 0.5\r\n",
      "58\t0\t0 0.5 0.5\r\n",
      "59\t1\t1 0.0 0.25\r\n",
      "60\t1\t0 0.5 0.5\r\n",
      "61\t1\t0 0.5 0.5\r\n",
      "62\t1\t0 0.5 0.5\r\n",
      "63\t0\t0 0.5 0.5\r\n",
      "64\t0\t0 0.5 0.5\r\n",
      "65\t0\t0 0.5 0.5\r\n",
      "66\t0\t0 0.5 0.5\r\n",
      "67\t0\t0 0.5 0.5\r\n",
      "68\t1\t0 0.5 0.5\r\n",
      "69\t0\t0 0.5 0.5\r\n",
      "70\t0\t0 0.5 0.5\r\n",
      "71\t0\t0 0.5 0.5\r\n",
      "72\t1\t0 0.5 0.5\r\n",
      "73\t1\t1 0.0 0.25\r\n",
      "74\t0\t0 0.5 0.5\r\n",
      "75\t0\t0 0.5 0.5\r\n",
      "76\t0\t0 0.5 0.5\r\n",
      "77\t1\t0 0.5 0.5\r\n",
      "78\t1\t0 0.5 0.5\r\n",
      "79\t1\t0 0.5 0.5\r\n",
      "80\t0\t0 0.5 0.5\r\n",
      "81\t0\t0 0.5 0.5\r\n",
      "82\t0\t0 0.5 0.5\r\n",
      "83\t0\t0 0.5 0.5\r\n",
      "84\t1\t0 0.5 0.5\r\n",
      "85\t1\t0 0.5 0.5\r\n",
      "86\t0\t0 0.5 0.5\r\n",
      "87\t0\t0 0.5 0.5\r\n",
      "88\t1\t0 0.5 0.5\r\n",
      "89\t1\t0 0.5 0.5\r\n",
      "90\t1\t0 0.5 0.5\r\n",
      "91\t1\t0 0.5 0.5\r\n",
      "92\t0\t0 0.5 0.5\r\n",
      "93\t0\t0 0.5 0.5\r\n",
      "94\t0\t0 0.5 0.5\r\n",
      "95\t1\t0 0.5 0.5\r\n",
      "96\t1\t0 0.5 0.5\r\n",
      "97\t1\t0 0.5 0.5\r\n",
      "98\t0\t0 0.5 0.5\r\n",
      "99\t1\t1 0.0 0.25\r\n",
      "100\t1\t1 0.0 0.25\r\n",
      "Accuracy:  60.0\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 assistance\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py \n",
    "\n",
    "   performs the multiple-word Naive Bayes classification via the chosen list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors are: \r\n",
      "1 1.0\r\n",
      "\r\n",
      "0 0.0\r\n",
      "\r\n",
      "\r\n",
      "Posteriors are: \r\n",
      "assistance in class 1 0.037037037037\r\n",
      "\r\n",
      "enlargementWithATypo in class 1 0.0\r\n",
      "\r\n",
      "valium in class 1 0.0\r\n",
      "\r\n",
      "DOC_ID | TRUTH | CLASS \r\n",
      "=======================\r\n",
      "\r\n",
      "1\t0\t1 0.0 1.0\r\n",
      "2\t0\t1 0.0 1.0\r\n",
      "3\t0\t1 0.0 1.0\r\n",
      "4\t0\t1 0.0 1.0\r\n",
      "5\t0\t1 0.0 1.0\r\n",
      "6\t0\t1 0.0 1.0\r\n",
      "7\t0\t1 0.0 1.0\r\n",
      "8\t0\t1 0.0 1.0\r\n",
      "9\t1\t1 0.0 1.0\r\n",
      "10\t1\t1 0.0 1.0\r\n",
      "11\t1\t1 0.0 0.037037037037\r\n",
      "12\t0\t1 0.0 1.0\r\n",
      "13\t0\t1 0.0 1.0\r\n",
      "14\t0\t1 0.0 1.0\r\n",
      "15\t0\t1 0.0 1.0\r\n",
      "16\t1\t1 0.0 1.0\r\n",
      "17\t1\t1 0.0 1.0\r\n",
      "18\t0\t1 0.0 0.037037037037\r\n",
      "19\t0\t1 0.0 1.0\r\n",
      "20\t0\t1 0.0 1.0\r\n",
      "21\t1\t1 0.0 1.0\r\n",
      "22\t1\t1 0.0 1.0\r\n",
      "23\t0\t1 0.0 0.037037037037\r\n",
      "24\t0\t1 0.0 1.0\r\n",
      "25\t0\t1 0.0 1.0\r\n",
      "26\t0\t1 0.0 1.0\r\n",
      "27\t1\t1 0.0 1.0\r\n",
      "28\t1\t1 0.0 1.0\r\n",
      "29\t0\t1 0.0 1.0\r\n",
      "30\t0\t1 0.0 1.0\r\n",
      "31\t0\t1 0.0 1.0\r\n",
      "32\t1\t1 0.0 1.0\r\n",
      "33\t1\t1 0.0 1.0\r\n",
      "34\t1\t1 0.0 1.0\r\n",
      "35\t0\t1 0.0 1.0\r\n",
      "36\t0\t1 0.0 1.0\r\n",
      "37\t0\t1 0.0 1.0\r\n",
      "38\t0\t1 0.0 1.0\r\n",
      "39\t1\t1 0.0 1.0\r\n",
      "40\t1\t1 0.0 1.0\r\n",
      "41\t0\t1 0.0 1.0\r\n",
      "42\t1\t1 0.0 1.0\r\n",
      "43\t1\t1 0.0 1.0\r\n",
      "44\t1\t1 0.0 1.0\r\n",
      "45\t1\t1 0.0 1.0\r\n",
      "46\t0\t1 0.0 1.0\r\n",
      "47\t0\t1 0.0 1.0\r\n",
      "48\t0\t1 0.0 1.0\r\n",
      "49\t0\t1 0.0 1.0\r\n",
      "50\t1\t1 0.0 1.0\r\n",
      "51\t1\t0 0.0 0.0\r\n",
      "52\t0\t1 0.0 1.0\r\n",
      "53\t0\t1 0.0 1.0\r\n",
      "54\t0\t1 0.0 1.0\r\n",
      "55\t1\t1 0.0 0.037037037037\r\n",
      "56\t1\t1 0.0 1.0\r\n",
      "57\t1\t1 0.0 1.0\r\n",
      "58\t0\t1 0.0 1.0\r\n",
      "59\t1\t1 0.0 0.037037037037\r\n",
      "60\t1\t1 0.0 1.0\r\n",
      "61\t1\t1 0.0 1.0\r\n",
      "62\t1\t1 0.0 1.0\r\n",
      "63\t0\t1 0.0 1.0\r\n",
      "64\t0\t1 0.0 1.0\r\n",
      "65\t0\t1 0.0 1.0\r\n",
      "66\t0\t1 0.0 1.0\r\n",
      "67\t0\t1 0.0 1.0\r\n",
      "68\t1\t1 0.0 1.0\r\n",
      "69\t0\t1 0.0 1.0\r\n",
      "70\t0\t1 0.0 1.0\r\n",
      "71\t0\t1 0.0 1.0\r\n",
      "72\t1\t1 0.0 1.0\r\n",
      "73\t1\t1 0.0 0.037037037037\r\n",
      "74\t0\t1 0.0 1.0\r\n",
      "75\t0\t1 0.0 1.0\r\n",
      "76\t0\t1 0.0 1.0\r\n",
      "77\t1\t1 0.0 1.0\r\n",
      "78\t1\t1 0.0 1.0\r\n",
      "79\t1\t1 0.0 1.0\r\n",
      "80\t0\t1 0.0 1.0\r\n",
      "81\t0\t1 0.0 1.0\r\n",
      "82\t0\t1 0.0 1.0\r\n",
      "83\t0\t1 0.0 1.0\r\n",
      "84\t1\t1 0.0 1.0\r\n",
      "85\t1\t1 0.0 1.0\r\n",
      "86\t0\t1 0.0 1.0\r\n",
      "87\t0\t1 0.0 1.0\r\n",
      "88\t1\t1 0.0 1.0\r\n",
      "89\t1\t1 0.0 1.0\r\n",
      "90\t1\t0 0.0 0.0\r\n",
      "91\t1\t1 0.0 1.0\r\n",
      "92\t0\t1 0.0 1.0\r\n",
      "93\t0\t1 0.0 1.0\r\n",
      "94\t0\t1 0.0 1.0\r\n",
      "95\t1\t1 0.0 1.0\r\n",
      "96\t1\t0 0.0 0.0\r\n",
      "97\t1\t1 0.0 1.0\r\n",
      "98\t0\t1 0.0 1.0\r\n",
      "99\t1\t1 0.0 0.037037037037\r\n",
      "100\t1\t1 0.0 0.037037037037\r\n",
      "Accuracy:  41.0\r\n"
     ]
    }
   ],
   "source": [
    "# 1.4\n",
    "\n",
    "!./pNaiveBayes.sh 3 \"assistance valium enlargementWithATypo\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After looking through the predictions for 1.3 and 1.4 more closely, it appears I wasn't able to make any positive predictions of spam. It is very likely that laplacian smoothing would greatly improve the accuracy as our vocabulary is extremely sparse compared to the entire vocabulary of the corpus. Furthermore, it appears that for reasons I wasn't able to figure out, I was unable to accurately count the lines that contained \"valium\". This is probably why the prediction for 1.4 is practically identical to 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
